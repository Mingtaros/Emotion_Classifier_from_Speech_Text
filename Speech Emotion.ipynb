{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f5cacf-5a59-4692-a557-83dba7cdeefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import torch\n",
    "import soundfile\n",
    "from utils import torch_device_seed, check_gpu, clear_device_cache\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec736367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gpu():\n",
    "    # torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "    is_cuda = torch.cuda.is_available()\n",
    "    \n",
    "    is_mps = torch.backends.mps.is_available()\n",
    "\n",
    "    # If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "    if is_cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"GPU is available\")\n",
    "    else:\n",
    "        if is_mps:\n",
    "            device = torch.device(\"mps\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"GPU not available, CPU used\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893a6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = check_gpu()\n",
    "torch_device_seed(3407)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ade1e-ff82-4c4f-a4de-8314a0b04199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waveforms(file):\n",
    "    wave,_= librosa.load(file,duration=3,offset=0.5,sr=sample_rate)\n",
    "    waveform, _ = librosa.effects.trim(wave,top_db=60) # tops at 60 decibels\n",
    "\n",
    "    # make sure waveform vectors are homogenous by defining explicitly\n",
    "    waveform_homo = np.zeros((int(sample_rate*3,)))\n",
    "    waveform_homo[:len(waveform)] = waveform\n",
    "    \n",
    "    # return a single file's waveform    \n",
    "    return waveform_homo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ba8b8-d5b2-4c72-9d7a-24ebfa4ff106",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_dict = {\n",
    "    0:'surprised',\n",
    "    1:'neutral',\n",
    "    2:'calm',\n",
    "    3:'happy',\n",
    "    4:'sad',\n",
    "    5:'angry',\n",
    "    6:'fearful',\n",
    "    7:'disgust'\n",
    "}\n",
    "# Additional attributes from RAVDESS to play with\n",
    "emotion_intensities = {\n",
    "    1: 'normal',\n",
    "    2: 'strong'\n",
    "}\n",
    "\n",
    "# RAVDESS native sample rate is 48k\n",
    "sample_rate = 48000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0ccd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features\n",
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate = sound_file.samplerate\n",
    "        stft = np.abs(librosa.stft(X)) if chroma else None\n",
    "        result=np.array([])\n",
    "        \n",
    "        if mfcc:\n",
    "            mfcc_vector = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40)\n",
    "            if len(mfcc_vector.shape) > 2:\n",
    "                mfcc_vector = mfcc_vector.squeeze(2)\n",
    "                mfcc_vector = mfcc_vector.reshape(40, -1) # n_mfcc parameter\n",
    "            mfccs = np.mean(mfcc_vector.T, axis=0)\n",
    "            result=np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma_vector = librosa.feature.chroma_stft(S=stft, sr=sample_rate)\n",
    "            if len(chroma_vector.shape) > 2:\n",
    "                chroma_vector = chroma_vector.squeeze(2)\n",
    "                chroma_vector = chroma_vector.reshape(12, -1) # n_chroma_bins parameter\n",
    "            chroma_feature = np.mean(chroma_vector.T, axis=0)\n",
    "            result=np.hstack((result, chroma_feature))\n",
    "        if mel:\n",
    "            mel_vector = librosa.feature.melspectrogram(y=X, sr=sample_rate)\n",
    "            if len(mel_vector.shape) > 2:\n",
    "                mel_vector = mel_vector.squeeze(2)\n",
    "                mel_vector = mel_vector.reshape(128, -1) # n_mels parameter\n",
    "            mel_feature = np.mean(mel_vector.T, axis=0)\n",
    "            result=np.hstack((result, mel_feature))\n",
    "        print(mfccs.shape, chroma_feature.shape, mel_feature.shape)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df110592-3d2d-4149-82c5-9007bd0b0a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAV = \"ravdess-emotional-speech-audio/\"\n",
    "\n",
    "dir_list = os.listdir(RAV)\n",
    "dir_list.sort()\n",
    "# features and labels\n",
    "emotions = []\n",
    "features = []\n",
    "# raw waveforms to augment later\n",
    "# waveforms = []\n",
    "# extra labels\n",
    "intensities, genders = [],[]\n",
    "\n",
    "for i in tqdm(dir_list, total=len(dir_list)):\n",
    "    if i == \".DS_Store\":\n",
    "        continue\n",
    "\n",
    "    if not i.startswith(\"Actor_\"):\n",
    "        continue\n",
    "\n",
    "    fname = os.listdir(RAV+i)\n",
    "    for f in fname:\n",
    "        # Modality (01 = full-AV, 02 = video-only, 03 = audio-only). We only have 03\n",
    "        # Vocal channel (01 = speech, 02 = song). We only have 01\n",
    "        # Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "        # Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
    "        # Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
    "        # Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "        # Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "        part = f.split('.')[0].split('-')\n",
    "        \n",
    "        emotion = int(part[2])\n",
    "        \n",
    "        file_dir = RAV + i +'/'+ f\n",
    "        print('file:',file_dir)\n",
    "        \n",
    "        #  move surprise to 0 for cleaner behaviour with PyTorch/0-indexing\n",
    "        if emotion == 8: \n",
    "            emotion = 0 # surprise is now at 0 index; other emotion indeces unchanged\n",
    "        elif emotion == 2: \n",
    "            continue # Skip calm emotion\n",
    "        \n",
    "        intensity = int(part[3])\n",
    "        \n",
    "        gender = int(part[6])\n",
    "        if gender%2==0:\n",
    "            gender = 'female'\n",
    "        else:\n",
    "            gender = 'male'\n",
    "        \n",
    "        # get waveform from the sample\n",
    "        # waveform = get_waveforms(file_dir)\n",
    "        \n",
    "        feature = extract_feature(file_dir, mfcc = True, chroma = True, mel = True)\n",
    "        # store waveforms and labels\n",
    "        features.append(feature)\n",
    "        emotions.append(emotion) # no need for the onehot encoded\n",
    "        intensities.append(intensity) # store intensity in case we wish to predict\n",
    "        genders.append(gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5e271-d0c7-446c-9004-8d41b31452e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emotions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea29a6c-e4ed-491d-a394-ef78913f9306",
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c15989f-59eb-4a0d-86e1-24a6335c9434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4deefa-1e7f-451d-857c-bef636bd819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid_test, y_train, y_valid_test = train_test_split(features, emotions, test_size=0.2, stratify=emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d6612-1bb7-457a-b30c-c9dc683a6d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d1139c-67c5-49e9-a881-8cc045273245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveformDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b90c0-f63a-430d-8076-b535c659abb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = WaveformDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_ds, batch_size=32, num_workers=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e51a8d-1907-4bec-bd21-c8c97d06674a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in train_dataloader:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c630be-c539-4f96-808c-6a7a7e7f8dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ds = WaveformDataset(X_valid_test, y_valid_test)\n",
    "valid_dataloader = DataLoader(valid_ds, batch_size=32, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3be430-6c21-414e-8d95-7ef055fcd5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcb3dc7-42fb-4730-9240-92c672dda5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SimpleLinearModel, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, 128, num_layers=1, dropout=0.1, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.linear1 = nn.Linear(128 * 2, 256) # for bidirectional\n",
    "        self.linear2 = nn.Linear(256, 128)\n",
    "        self.linear3 = nn.Linear(128, output_size)\n",
    "\n",
    "    \n",
    "    def forward(self, waveform):\n",
    "        output, _ = self.lstm(waveform)\n",
    "        output = F.relu(self.linear1(output))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb5bdd-a040-4f98-82c5-ad6f2e0ac14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "clear_device_cache()\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb8d627-141b-4994-aa5c-748295ad1252",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = SimpleLinearModel(features[0].shape[0], len(emotions_dict)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(linear_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ecaf30-c22f-4a71-9722-aa9c6ad276fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd157a-9913-4627-9571-6253a508fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, criterion, optimizer, epochs=EPOCHS):\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):  # Loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        model.train()\n",
    "        for inputs, labels in train_dataloader:\n",
    "            inputs = inputs.to(device).float()\n",
    "            labels = labels.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            running_loss += loss.item() \n",
    "\n",
    "            _, predicted = torch.max(outputs.data, axis=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "        # get train loss and accuracy\n",
    "        train_loss = running_loss / len(train_dataloader.dataset)\n",
    "        train_accuracy = correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "    \n",
    "        # get test loss and accuracy\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for data in valid_dataloader:\n",
    "                inputs, labels = data\n",
    "                inputs = inputs.to(device).float()\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, axis=1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(valid_dataloader)\n",
    "        val_accuracy = val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch}: train_loss: {train_loss:.4f}; train_accuracy: {train_accuracy:.4f}; val_loss: {val_loss:.4f}; val_accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"train_loss\": train_losses,\n",
    "        \"train_accuracy\": train_accuracies,\n",
    "        \"val_loss\": val_losses,\n",
    "        \"val_accuracy\": val_accuracies\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8000cc25-5baa-4e56-a6d7-3d799c618a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = fit(linear_model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d430e54-d39a-44db-8358-8d3a4751e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2161817-9f7d-4383-853a-bed90b8e4398",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 22050\n",
    "\n",
    "y_sweep = librosa.chirp(fmin=librosa.note_to_hz('C#4'),\n",
    "                        fmax=librosa.note_to_hz('Eb5'),\n",
    "                        sr=sr,\n",
    "                        duration=1)\n",
    "\n",
    "display(Audio(data=y_sweep, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1629cf54-fb81-47d7-8525-d8601e9abee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_dataloader:\n",
    "    print(x[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbd9ab6-7770-4ea3-83b4-2f6e071e58f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, filename):\n",
    "    f = filename.split(\"/\")[-1]\n",
    "    part = f.split('.')[0].split('-')\n",
    "    emotion = int(part[2])\n",
    "\n",
    "    emotion_label = emotions_dict[emotion]\n",
    "    this_audio, sr = librosa.load(filename)\n",
    "\n",
    "    feature = extract_feature(filename, mfcc = True, chroma = True, mel = True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        feature_tensor = torch.tensor(np.array([feature]), dtype=torch.float).to(device)\n",
    "        output = model(feature_tensor)\n",
    "        _, prediction = torch.max(output, axis=1)\n",
    "        prediction_label = emotions_dict[prediction.cpu().numpy()[0]]\n",
    "\n",
    "    # show\n",
    "    display(Audio(data=this_audio, rate=sr))\n",
    "    print(f\"Label     : {emotion_label}\")\n",
    "    print(f\"Prediction: {prediction_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb45dc2-bddd-4809-a0d0-70c65ec63e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(linear_model, \"ravdess-emotional-speech-audio/Actor_24/03-01-07-01-01-02-24.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
